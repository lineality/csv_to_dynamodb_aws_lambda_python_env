{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Datatype_Tester_component_minimal_v5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGcpkDG9-MOh"
      },
      "source": [
        "# This is a notebook/.py script to test the datatypes of columns in your .csv files\n",
        "\n",
        "Goal: You want the AWS data-tables created with your .csv to accurately reflect data-types for the data.\n",
        "\n",
        "These are the main datatype you will use for AWS-DynamoDB:\n",
        "- N for all numbers \n",
        "- S for strings\n",
        "- BOOL for boolean (True / False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEAR-5J5_O0_"
      },
      "source": [
        "# Instructions:\n",
        "1. upload/drag-and-drop one .csv file into this notebook (select the files tab on the right (last of the icons from the top), then drag and drop)\n",
        "\n",
        "Note: if your .csv file is too big to load into colab, you can use the '.py' version, or download the .ipynb (file -> download) version. Run all cells and you will see 'datatype_tester.py' file in the directory (download by clicking on the 3-dots in the file browser on the left).\n",
        "Note: if you run the .ipynb or .py locally, use in env with pandas installed.\n",
        "\n",
        "(or you could run this .ipnb file locally in jupyter...)\n",
        "\n",
        "2. Run all cells: select \"Run all\" from the Runtime menu above.\n",
        "3. examine the output and look for problems \n",
        "\n",
        "(data types that are not correct, or the program crashes due to datatype issues) e.g. Does pandas report 'object' but it looks like a number to you? If there is text in some cells (e.g. \"NONE\") then the column is mis-identified because there is a text string mixed into the number-column.\n",
        "4. look at (double click on) the .csv meta-data files that are created when you run the notebook (metadata_aws.csv & metadata_pandas.csv) and look for data-type problems.\n",
        "5. Do all data-types look to be accurately reflected?\n",
        "6. Are there problems with missing data? (null/na/nan, etc?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJaFZsqc_A_5"
      },
      "source": [
        "## code to auto detect and run test on .csv files\n",
        "( you can keep this hidden while running) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0QFzaBOHh7M"
      },
      "source": [
        "\n",
        "this_code = \"\"\" ## Helper Colab cell to load a .csv and create a draft of a metadata-csv\n",
        "\n",
        "# import library\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "# get name of file from user\n",
        "# name_of_csv = input(\"What is the name of your .csv?\")\n",
        "\n",
        "# helper function \n",
        "def make_metadata_csv(name_of_csv):\n",
        "    '''\n",
        "    This function makes a metadata_ file\n",
        "    with three kinds of data:\n",
        "    1. the pandas datatype for each column\n",
        "    2. the AWS dynamoDB data type for each column\n",
        "    3. an example of the data for each column\n",
        "\n",
        "    Requires: pandas as pd\n",
        "    '''\n",
        "\n",
        "    # load file into pandas dataframe\n",
        "    df = pd.read_csv( name_of_csv )\n",
        "\n",
        "    # extract list of column names from .csv\n",
        "    column_name_list = list(df.columns)\n",
        "\n",
        "    # # inspection\n",
        "    # print(column_name_list)\n",
        "\n",
        "    # make empty list for datatypes, the same size as the name-list\n",
        "    pandas_dtypes_list = [None]*len(column_name_list)\n",
        "    AWS_dtypes_list = [None]*len(column_name_list)\n",
        "\n",
        "    # make a list of example items for inspection\n",
        "    example_item_list = list(df.loc[0])\n",
        "\n",
        "    # extract the datatype of each column as recognized by pandas\n",
        "    for index, column_name in enumerate(column_name_list):\n",
        "        pandas_dtypes_list[index] = str(df[column_name].dtypes)\n",
        "\n",
        "    # # inspection\n",
        "    # print(pandas_dtypes_list)\n",
        "\n",
        "    # conversion_dictionary\n",
        "    conversion_dict = {\n",
        "        \"object\" : 'S',\n",
        "        \"int64\" : 'N',\n",
        "        \"float64\" : 'N',\n",
        "        \"datetime64\" : 'S',\n",
        "        \"bool\" : 'BOOL',\n",
        "    }\n",
        "\n",
        "    # convert to AWS-DynamoDB datatypes\n",
        "    # look up each item in converstion dictionary\n",
        "    # and put AWS value in new list\n",
        "    for index, column_name in enumerate(pandas_dtypes_list):\n",
        "        AWS_dtypes_list[index] = conversion_dict[column_name]\n",
        "\n",
        "    # make a dictionary of lists \n",
        "    type_dict = {'column_name': column_name_list, \n",
        "                 'AWS_column_dtype': AWS_dtypes_list, \n",
        "                 'pandas_column_dtype': pandas_dtypes_list,\n",
        "                 'example_item_list': example_item_list,\n",
        "                 } \n",
        "        \n",
        "    # make a new pandas dataframe based on the dictionary of lists    \n",
        "    df_meta = pd.DataFrame(type_dict)\n",
        "        \n",
        "    # make file name for csv meta_data file (for AWS or for normal OS)\n",
        "    # slice to remove the first /tmp/ name\n",
        "    # new_name_of_csv = \"/tmp/metadata_\" + name_of_csv[5:]\n",
        "\n",
        "    # # for normal OS or python notebook\n",
        "    new_name_of_csv = \"metadata_\" + name_of_csv\n",
        "\n",
        "    '''\n",
        "    Below are two different versions of formatted output\n",
        "    in terms of the structure\n",
        "    of the resulting .csv file\n",
        "    '''\n",
        "\n",
        "    # # saving the dataframe\n",
        "    # df_meta.to_csv( new_name_of_csv )\n",
        "\n",
        "    # # saving the dataframe (alternate version)\n",
        "    df_meta.to_csv( new_name_of_csv , header=True, index=False)\n",
        "\n",
        "    # delete dataframes to save memory\n",
        "    #del df\n",
        "    #del df_meta\n",
        "\n",
        "    # end program\n",
        "    return None\n",
        "\n",
        "\n",
        "# Helper Function\n",
        "def make_primary_key_warning_flag_list(file_name):\n",
        "\n",
        "    # load file into pandas dataframe\n",
        "    df = pd.read_csv( file_name )\n",
        "\n",
        "    #############\n",
        "    # Make Flags\n",
        "    #############\n",
        "\n",
        "    mixed_datatype_flag = False\n",
        "    missing_data_flag = False\n",
        "    duplicate_data_flag = False\n",
        "\n",
        "    ############################\n",
        "    # check mixed_datatype_flag\n",
        "    ############################\n",
        "\n",
        "    if str(df.iloc[:, 0].dtype) == 'object':\n",
        "        mixed_datatype_flag = True\n",
        "\n",
        "    # # for terminal or inspection\n",
        "    # print( mixed_datatype_flag )\n",
        "\n",
        "    ############################\n",
        "    # check missing_data_flag\n",
        "    ############################\n",
        "\n",
        "    # check is na\n",
        "    if df.iloc[:, 0].isna().sum() != 0:\n",
        "        missing_data_flag = True\n",
        "\n",
        "    # check is null\n",
        "    if df.iloc[:, 0].isnull().sum() != 0:\n",
        "        missing_data_flag = True\n",
        "\n",
        "    # # for terminal or inspection\n",
        "    # print( missing_data_flag )\n",
        "\n",
        "    ############################\n",
        "    # check duplicate_data_flag\n",
        "    ############################\n",
        "\n",
        "    if ( df.iloc[:, 0].value_counts().sum() == len(df.iloc[:, 0].value_counts()) ) == False:\n",
        "        duplicate_data_flag = True\n",
        "\n",
        "    # # for terminal or inspection\n",
        "    # print( duplicate_data_flag )\n",
        "\n",
        "    #####################\n",
        "    # Make list of flags\n",
        "    #####################\n",
        "\n",
        "    warning_flag_list = []\n",
        "\n",
        "    if mixed_datatype_flag == True:\n",
        "        warning_flag_list.append( 'mixed_datatype_flag' )\n",
        "    if missing_data_flag == True:\n",
        "        warning_flag_list.append( 'missing_data_flag' )\n",
        "    if duplicate_data_flag == True:\n",
        "        warning_flag_list.append( 'duplicate_data_flag' )\n",
        "\n",
        "    ###############################\n",
        "    # return list of warning flags\n",
        "    ###############################\n",
        "\n",
        "    return print(warning_flag_list)\n",
        "\n",
        "# in the case of a long wait, give the user some idea\n",
        "# of the progress through the files (crude but works)\n",
        "progress_counter = 0\n",
        "# inspection\n",
        "# print(\"progress Counter:\")\n",
        "\n",
        "# TODO\n",
        "# import glob \n",
        "list_of_csv_files = glob.glob('*.csv', recursive = True)\n",
        "\n",
        "# iterate through all .rds files in directory\n",
        "the_path = \".\"\n",
        "for filename in list_of_csv_files:\n",
        "\n",
        "    # inspection\n",
        "    # print(filename)\n",
        "\n",
        "    # find AWS data types\n",
        "    make_metadata_csv(filename)\n",
        "\n",
        "    # make \n",
        "    make_primary_key_warning_flag_list(file_name)\n",
        "\n",
        "    # Show Progress:\n",
        "    progress_counter += 1\n",
        "    print(f\"{ progress_counter }/{ len(list_of_csv_files) }\")\n",
        "\n",
        "# list of metadata files\n",
        "list_of_metadata_files = glob.glob('metadata_*.csv', recursive = True)\n",
        "\n",
        "\n",
        "# Yay!!\n",
        "print(\"All Done!!\")\n",
        "\n",
        "# may take extra time to print\n",
        "# print( \"List of new files = \\n\", glob.glob('metadata_*.csv', recursive = True) )\n",
        "\"\"\"\n",
        "\n",
        "with open('datatype_tester.py', \"w\") as file_object:\n",
        "   # read file content\n",
        "   file_object.write( this_code )"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAOIs21sAxVm",
        "outputId": "658da74a-4c7e-449e-8e48-717c5c9316fd"
      },
      "source": [
        "## Helper Colab cell to load a .csv and create a draft of a metadata-csv\n",
        "\n",
        "# import library\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "# get name of file from user\n",
        "# name_of_csv = input(\"What is the name of your .csv?\")\n",
        "\n",
        "# helper function \n",
        "def make_metadata_csv(name_of_csv):\n",
        "    \"\"\"\n",
        "    This function makes a metadata_ file\n",
        "    with three kinds of data:\n",
        "    1. the pandas datatype for each column\n",
        "    2. the AWS dynamoDB data type for each column\n",
        "    3. an example of the data for each column\n",
        "\n",
        "    Requires: pandas as pd\n",
        "    \"\"\"\n",
        "\n",
        "    # load file into pandas dataframe\n",
        "    df = pd.read_csv( name_of_csv )\n",
        "\n",
        "    # extract list of column names from .csv\n",
        "    column_name_list = list(df.columns)\n",
        "\n",
        "    # # inspection\n",
        "    # print(column_name_list)\n",
        "\n",
        "    # make empty list for datatypes, the same size as the name-list\n",
        "    pandas_dtypes_list = [None]*len(column_name_list)\n",
        "    AWS_dtypes_list = [None]*len(column_name_list)\n",
        "\n",
        "    # make a list of example items for inspection\n",
        "    example_item_list = list(df.loc[0])\n",
        "\n",
        "    # extract the datatype of each column as recognized by pandas\n",
        "    for index, column_name in enumerate(column_name_list):\n",
        "        pandas_dtypes_list[index] = str(df[column_name].dtypes)\n",
        "\n",
        "    # # inspection\n",
        "    # print(pandas_dtypes_list)\n",
        "\n",
        "    # conversion_dictionary\n",
        "    conversion_dict = {\n",
        "        \"object\" : 'S',\n",
        "        \"int64\" : 'N',\n",
        "        \"float64\" : 'N',\n",
        "        \"datetime64\" : 'S',\n",
        "        \"bool\" : 'BOOL',\n",
        "    }\n",
        "\n",
        "    # convert to AWS-DynamoDB datatypes\n",
        "    # look up each item in converstion dictionary\n",
        "    # and put AWS value in new list\n",
        "    for index, column_name in enumerate(pandas_dtypes_list):\n",
        "        AWS_dtypes_list[index] = conversion_dict[column_name]\n",
        "\n",
        "    # make a dictionary of lists \n",
        "    type_dict = {'column_name': column_name_list, \n",
        "                 'AWS_column_dtype': AWS_dtypes_list, \n",
        "                 'pandas_column_dtype': pandas_dtypes_list,\n",
        "                 'example_item_list': example_item_list,\n",
        "                 } \n",
        "        \n",
        "    # make a new pandas dataframe based on the dictionary of lists    \n",
        "    df_meta = pd.DataFrame(type_dict)\n",
        "        \n",
        "    # make file name for csv meta_data file (for AWS or for normal OS)\n",
        "    # slice to remove the first /tmp/ name\n",
        "    # new_name_of_csv = \"/tmp/metadata_\" + name_of_csv[5:]\n",
        "\n",
        "    # # for normal OS or python notebook\n",
        "    new_name_of_csv = \"metadata_\" + name_of_csv\n",
        "\n",
        "    \"\"\"\n",
        "    Below are two different versions of formatted output\n",
        "    in terms of the structure\n",
        "    of the resulting .csv file\n",
        "    \"\"\"\n",
        "\n",
        "    # # saving the dataframe\n",
        "    # df_meta.to_csv( new_name_of_csv )\n",
        "\n",
        "    # # saving the dataframe (alternate version)\n",
        "    df_meta.to_csv( new_name_of_csv , header=True, index=False)\n",
        "\n",
        "    # delete dataframes to save memory\n",
        "    #del df\n",
        "    #del df_meta\n",
        "\n",
        "    # end program\n",
        "    return None\n",
        "\n",
        "\n",
        "# Helper Function\n",
        "def make_primary_key_warning_flag_list(file_name):\n",
        "\n",
        "    # load file into pandas dataframe\n",
        "    df = pd.read_csv( file_name )\n",
        "\n",
        "    #############\n",
        "    # Make Flags\n",
        "    #############\n",
        "\n",
        "    mixed_datatype_flag = False\n",
        "    missing_data_flag = False\n",
        "    duplicate_data_flag = False\n",
        "\n",
        "    ############################\n",
        "    # check mixed_datatype_flag\n",
        "    ############################\n",
        "\n",
        "    if str(df.iloc[:, 0].dtype) == 'object':\n",
        "        mixed_datatype_flag = True\n",
        "\n",
        "    # # for terminal or inspection\n",
        "    # print( mixed_datatype_flag )\n",
        "\n",
        "    ############################\n",
        "    # check missing_data_flag\n",
        "    ############################\n",
        "\n",
        "    # check is na\n",
        "    if df.iloc[:, 0].isna().sum() != 0:\n",
        "        missing_data_flag = True\n",
        "\n",
        "    # check is null\n",
        "    if df.iloc[:, 0].isnull().sum() != 0:\n",
        "        missing_data_flag = True\n",
        "\n",
        "    # # for terminal or inspection\n",
        "    # print( missing_data_flag )\n",
        "\n",
        "    ############################\n",
        "    # check duplicate_data_flag\n",
        "    ############################\n",
        "\n",
        "    if ( df.iloc[:, 0].value_counts().sum() == len(df.iloc[:, 0].value_counts()) ) == False:\n",
        "        duplicate_data_flag = True\n",
        "\n",
        "    # # for terminal or inspection\n",
        "    # print( duplicate_data_flag )\n",
        "\n",
        "    #####################\n",
        "    # Make list of flags\n",
        "    #####################\n",
        "\n",
        "    warning_flag_list = []\n",
        "\n",
        "    if mixed_datatype_flag == True:\n",
        "        warning_flag_list.append( 'mixed_datatype_flag' )\n",
        "    if missing_data_flag == True:\n",
        "        warning_flag_list.append( 'missing_data_flag' )\n",
        "    if duplicate_data_flag == True:\n",
        "        warning_flag_list.append( 'duplicate_data_flag' )\n",
        "\n",
        "    ###############################\n",
        "    # return list of warning flags\n",
        "    ###############################\n",
        "\n",
        "    return print(f\"file name: {filename}, warning: {warning_flag_list}\")\n",
        "\n",
        "# in the case of a long wait, give the user some idea\n",
        "# of the progress through the files (crude but works)\n",
        "progress_counter = 0\n",
        "# inspection\n",
        "# print(\"progress Counter:\")\n",
        "\n",
        "# TODO\n",
        "# import glob \n",
        "list_of_csv_files = glob.glob('*.csv', recursive = True)\n",
        "\n",
        "# iterate through all .rds files in directory\n",
        "the_path = \".\"\n",
        "for filename in list_of_csv_files:\n",
        "\n",
        "    # inspection\n",
        "    # print(filename)\n",
        "\n",
        "    # find AWS data types\n",
        "    make_metadata_csv(filename)\n",
        "\n",
        "    # make \n",
        "    make_primary_key_warning_flag_list(filename)\n",
        "\n",
        "    # Show Progress:\n",
        "    progress_counter += 1\n",
        "    print(f\"{ progress_counter }/{ len(list_of_csv_files) }\")\n",
        "\n",
        "# list of metadata files\n",
        "list_of_metadata_files = glob.glob('metadata_*.csv', recursive = True)\n",
        "\n",
        "\n",
        "# Yay!!\n",
        "print(\"All Done!!\")\n",
        "\n",
        "# may take extra time to print\n",
        "# print( \"List of new files = \\n\", glob.glob('metadata_*.csv', recursive = True) )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file name: sm_salary2.csv, warning: []\n",
            "1/3\n",
            "file name: sm_salary3_test_missing.csv, warning: ['missing_data_flag']\n",
            "2/3\n",
            "file name: sm_salary.csv, warning: []\n",
            "3/3\n",
            "All Done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOJ_ew0ERo9"
      },
      "source": [
        "# Look at original\n",
        "- Look at your original data.\n",
        "- Does the listed datatype reflect what appears in the list? (e.g. is a number reported to be an 'object')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1_zfvLMEvLY",
        "outputId": "b3554155-d043-4933-c2e0-8b9814cdb362"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datatype_tester.py\t\t      sample_data\n",
            "metadata_sm_salary2.csv\t\t      sm_salary2.csv\n",
            "metadata_sm_salary3_test_missing.csv  sm_salary3_test_missing.csv\n",
            "metadata_sm_salary.csv\t\t      sm_salary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "at_nWnbmET1L",
        "outputId": "ec119b34-d1c5-478d-f7f0-60c7f05a28d2"
      },
      "source": [
        "# load file into pandas dataframe\n",
        "try:\n",
        "    df = pd.read_csv( list_of_csv_files[0] )\n",
        "\n",
        "except:\n",
        "    df = pd.read_csv( list_of_csv_files[0] , encoding = \"ISO-8859-1\" )\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>YearsExperience</th>\n",
              "      <th>Salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>39343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>46205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>37731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>43525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2.2</td>\n",
              "      <td>39891</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   row_id  YearsExperience  Salary\n",
              "0       1              1.1   39343\n",
              "1       2              1.3   46205\n",
              "2       3              1.5   37731\n",
              "3       4              2.0   43525\n",
              "4       5              2.2   39891"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OThaPv3pFAiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a987242f-1774-477f-9bdf-25d3cc19b9f4"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "row_id               int64\n",
              "YearsExperience    float64\n",
              "Salary               int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqVRZ5leFa8S"
      },
      "source": [
        "# How many 'null/na/nan' rows are there?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjmu4r2xFPD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f1e923-2823-40fa-eadd-bebfcf6adbc5"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "row_id             0\n",
              "YearsExperience    0\n",
              "Salary             0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgv6Wu8TFWqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8494f822-9749-48eb-eeeb-eb782df7f479"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "row_id             0\n",
              "YearsExperience    0\n",
              "Salary             0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBRC6-9qFiEH"
      },
      "source": [
        "## Can you check for other impossible values such as impossible zero?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfLrh289CglQ"
      },
      "source": [
        "# Visualize AWS & Pandas datatypes + Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w9USkP-LtPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0860fcc4-0014-434d-b411-b68adf16bb1f"
      },
      "source": [
        "list_of_metadata_files"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['metadata_sm_salary2.csv',\n",
              " 'metadata_sm_salary3_test_missing.csv',\n",
              " 'metadata_sm_salary.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1r1SEvqCfdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "1ac429ec-1324-462a-cbd8-d86d3de1161a"
      },
      "source": [
        "meta_df = pd.read_csv(list_of_metadata_files[0])\n",
        "meta_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>column_name</th>\n",
              "      <th>AWS_column_dtype</th>\n",
              "      <th>pandas_column_dtype</th>\n",
              "      <th>example_item_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>row_id</td>\n",
              "      <td>N</td>\n",
              "      <td>int64</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>YearsExperience</td>\n",
              "      <td>N</td>\n",
              "      <td>float64</td>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Salary</td>\n",
              "      <td>N</td>\n",
              "      <td>int64</td>\n",
              "      <td>39343.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       column_name AWS_column_dtype pandas_column_dtype  example_item_list\n",
              "0           row_id                N               int64                1.0\n",
              "1  YearsExperience                N             float64                1.1\n",
              "2           Salary                N               int64            39343.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}
